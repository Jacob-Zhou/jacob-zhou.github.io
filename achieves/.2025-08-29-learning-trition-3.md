---
layout:     post
title:      "Triton 学习手记 （四）：实现 Flash Attention 2"
description: "基于 Triton 实现 Flash Attention 2 的实践手记"
date:       2025-08-29 19:00:0+0800
author:     "Houquan Zhou"
header-img: "/assets/img/post-bg.jpg"
mathjax: true
catalog: true
hidden: true
tags:
    - Triton
    - 手记
---

## 前言

在[之前的手记](/2025/08/19/learning-trition-0)中，我们介绍了 Triton 的基本概念，在这篇手记中我们开始实践，实现 Flash Attention 2。

在实现 Flash Attention 2 的同时我们也会对 Triton 代码的结构、控制流等进行介绍。

## 什么是 Flash Attention 2？

在进入实现环节之前，我们先来回顾一下 Flash Attention 2。
如果读者熟悉 Flash Attention 2，可以跳过这一节。([直达下一节](#flash-attention-2-的-triton-实现))

### Multi-Head Attention

Multi-Head Attention 机制由 《Attention is all you need》 一文提出，从 NLP 领域开始迅速流行，逐渐占据了各个领域。
目前主流的大模型架构都采用了 Multi-Head Attention 机制。

Multi-Head Attention 的计算公式如下：

$$
\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{d_k}}\right)\boldsymbol{V}
$$

其中，$\boldsymbol{Q} \in \mathbb{R}^{B \times L \times H \times d}$，$\boldsymbol{K} \in \mathbb{R}^{B \times L \times H \times d}$，$\boldsymbol{V} \in \mathbb{R}^{B \times L \times H \times d_v}$，$B$ 是 batch size，$L$ 是序列长度，$H$ 是 head 的数量，$d$ 是查询和键向量的维度，$d_v$ 是值向量的维度。


### 加速 Multi-Head Attention

我们现在来考虑如何加速 Multi-Head Attention 的计算。
第一个需要考虑的是哪些地方我们可以并行化。

显然，在 Batch 中，不同样本的计算是独立的，在 Batch 维度上，我们可以并行化。
另外，在 Head 维度上，不同 Head 的计算也是互不影响的，因此我们在 Head 维度上也可以并行化。

## 使用 Triton 自定义 PyTorch 操作

```python
@triton.jit
def kernel_fwd(
    x_ptr,
    N: tl.constexpr,
    M: tl.constexpr
):
    pass

@triton.jit
def kernel_bwd(
    x_ptr,
    N: tl.constexpr,
    M: tl.constexpr
):
    pass

def helper_fwd(x: torch.Tensor):
    pass

def helper_bwd(dy: torch.Tensor):
    pass

class Function(torch.autograd.Function):

    @staticmethod
    @input_guard  # 用于确保输入的 Tensor 是连续的
    def forward(ctx, input: torch.Tensor):
        return helper_fwd(input)

    @staticmethod
    @input_guard
    def backward(ctx, dy: torch.Tensor):
        dx = helper_bwd(dy)
        return dx, None, None

def fn(input: torch.Tensor):
    return Function.apply(input)
```

## Flash Attention 2 的 Triton 实现
